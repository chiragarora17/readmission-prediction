{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "1) Drop unwanted columns : patient_nbr, encounter_id, weight, payer_code, medical_specialty.\n",
    "\n",
    "2) Drop columns 'citoglipton', 'examide' which has the same values across all the rows. So these columns won't help us in modeling\n",
    "\n",
    "2) cleaned up race column ( replaced ? to another category and applied LabelEncoder).\n",
    "\n",
    "3) drop rows which has invalid gender.\n",
    "\n",
    "4) cleaned age column by removing the interval and putting the median value. \n",
    "\n",
    "5) assigned 3 categories to admission_type_id column\n",
    "\n",
    "6) assigned 3 categories to admission_source_id column\n",
    "\n",
    "7) remove any rows where the patient is expired based on discharge_disposition_id = 11\n",
    "\n",
    "8) cleaning up the max glu serum into 3 categories\n",
    "\n",
    "9) cleaning up the A1Cresult\n",
    "\n",
    "10) cleaning up the diag columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File diabetic_data.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b4e20a382ac6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"diabetic_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mindex_mapping\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"IDs_mapping.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bhaktishah/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bhaktishah/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bhaktishah/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bhaktishah/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bhaktishah/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File diabetic_data.csv does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df = pd.read_csv(\"dataset_diabetes/diabetic_data.csv\")\n",
    "index_mapping =  pd.read_csv(\"dataset_diabetes/IDs_mapping.csv\")\n",
    "\n",
    "\n",
    "def replaceColumn(df, col, oldval, newval):\n",
    "    df[col] = df[col].replace(oldval, newval)\n",
    "    return df\n",
    "\n",
    "def replaceColumnList(df, col, listOfOldVal, newval):\n",
    "    newDf = df\n",
    "    for oldVal in listOfOldVal:\n",
    "        newDf = replaceColumn(df, col, oldVal, newval)\n",
    "    return newDf\n",
    "\n",
    "def transformLabelEncoder(df, col):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    return df;\n",
    "\n",
    "def scale(df, col):\n",
    "    x = df[[col]].values.astype(int)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    df[col] = x_scaled\n",
    "    return df;\n",
    "\n",
    "def diagColn(df):\n",
    "    colList = ['diag_1','diag_2','diag_3']\n",
    "    for col in colList:\n",
    "        df.loc[df[col].str.contains('E'), col] = '0'\n",
    "        df.loc[df[col].str.contains('V'), col] = '0'\n",
    "        df = replaceColumn(df, col, '?', -1)\n",
    "        df = replaceColumn(df, col, '0', 0)\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "\n",
    "        df[col] = np.where(df[col].between(1, 139, inclusive=True), 1, df[col])\n",
    "        df[col] = np.where(df[col].between(140, 239, inclusive=True), 2, df[col])\n",
    "        df[col] = np.where(df[col].between(240, 279, inclusive=True), 3, df[col])\n",
    "        df[col] = np.where(df[col].between(280, 289, inclusive=True), 4, df[col])\n",
    "        df[col] = np.where(df[col].between(290, 319, inclusive=True), 5, df[col])\n",
    "        df[col] = np.where(df[col].between(320, 389, inclusive=True), 6, df[col])\n",
    "        df[col] = np.where(df[col].between(390, 459, inclusive=True), 7, df[col])\n",
    "        df[col] = np.where(df[col].between(460, 519, inclusive=True), 8, df[col])\n",
    "        df[col] = np.where(df[col].between(520, 579, inclusive=True), 9, df[col])\n",
    "        df[col] = np.where(df[col].between(580, 629, inclusive=True), 10, df[col])\n",
    "        df[col] = np.where(df[col].between(630, 679, inclusive=True), 11, df[col])\n",
    "        df[col] = np.where(df[col].between(680, 709, inclusive=True), 12, df[col])\n",
    "        df[col] = np.where(df[col].between(710, 739, inclusive=True), 13, df[col])\n",
    "        df[col] = np.where(df[col].between(740, 759, inclusive=True), 14, df[col])\n",
    "        df[col] = np.where(df[col].between(760, 779, inclusive=True), 15, df[col])\n",
    "        df[col] = np.where(df[col].between(780, 799, inclusive=True), 16, df[col])\n",
    "        df[col] = np.where(df[col].between(800, 999, inclusive=True), 17, df[col])\n",
    "    return df\n",
    "\n",
    "# drop few columns such as patient_nbr, encounter_id, weight, payer_code, medical_specialty\n",
    "df = df.drop(['patient_nbr', 'encounter_id', 'weight', 'payer_code', 'medical_specialty', 'citoglipton', 'examide'], axis=1)\n",
    "\n",
    "# fixing race column\n",
    "df = replaceColumn(df, 'race', '?', \"unknown\")\n",
    "df = transformLabelEncoder(df, \"race\")\n",
    "\n",
    "# dropping rows where gender is invalid\n",
    "df = df[df.gender != 'Unknown/Invalid']\n",
    "\n",
    "# fixing age column\n",
    "# TODO maybe send a list of tuple to do mass replace in 1 shot\n",
    "df = replaceColumn(df, 'age', '[0-10)', 4)\n",
    "df = replaceColumn(df, 'age', '[10-20)', 14)\n",
    "df = replaceColumn(df, 'age', '[20-30)', 24)\n",
    "df = replaceColumn(df, 'age', '[30-40)', 34)\n",
    "df = replaceColumn(df, 'age', '[40-50)', 44)\n",
    "df = replaceColumn(df, 'age', '[50-60)', 54)\n",
    "df = replaceColumn(df, 'age', '[60-70)', 64)\n",
    "df = replaceColumn(df, 'age', '[70-80)', 74)\n",
    "df = replaceColumn(df, 'age', '[80-90)', 84)\n",
    "df = replaceColumn(df, 'age', '[90-100)', 94)\n",
    "\n",
    "# fixing admission_type_id column\n",
    "df = replaceColumnList(df, 'admission_type_id', [2,7], 1)\n",
    "df = replaceColumnList(df, 'admission_type_id', [6,8], 5)\n",
    "df = replaceColumnList(df, 'admission_type_id', [4], 3)\n",
    "\n",
    "# fixing admission_source_id column\n",
    "df = replaceColumnList(df, 'admission_source_id', [2,3], 1)\n",
    "df = replaceColumnList(df, 'admission_source_id', [5,6,22], 4)\n",
    "df = replaceColumnList(df, 'admission_source_id', [10,25], 7)\n",
    "df = replaceColumnList(df, 'admission_source_id', [17,20], 9)\n",
    "df = replaceColumnList(df, 'admission_source_id', [13,14], 11)\n",
    "\n",
    "print('admission_type_id', df['admission_type_id'][df['admission_type_id'] == 7].count())\n",
    "print('admission_source_id', df['admission_source_id'][df['admission_source_id'] == 2].count())\n",
    "print('discharge_disposition_id', df['discharge_disposition_id'][df['discharge_disposition_id'] == 11].count())\n",
    "\n",
    "# dropping people who expired already\n",
    "df = df[df.discharge_disposition_id != 11]\n",
    "\n",
    "print('discharge_disposition_id', df['discharge_disposition_id'][df['discharge_disposition_id'] == 11].count())\n",
    "\n",
    "# cleaning up the max glu serum\n",
    "df = replaceColumnList(df, 'max_glu_serum', ['>300', '>200'], 1)\n",
    "df = replaceColumnList(df, 'max_glu_serum', ['Norm'], 0)\n",
    "df = replaceColumnList(df, 'max_glu_serum', ['None'], -1)\n",
    "\n",
    "print('max_glu_serum : 1', df['max_glu_serum'][df['max_glu_serum'] == 1].count())\n",
    "print('max_glu_serum : 0', df['max_glu_serum'][df['max_glu_serum'] == 0].count())\n",
    "\n",
    "# cleaning up the A1Cresult\n",
    "df = replaceColumnList(df, 'A1Cresult', ['>7', '>8'], 1)\n",
    "df = replaceColumnList(df, 'A1Cresult', ['Norm'], 0)\n",
    "df = replaceColumnList(df, 'A1Cresult', ['None'], -1)\n",
    "\n",
    "print('A1Cresult : 1', df['A1Cresult'][df['A1Cresult'] == 1].count())\n",
    "print('A1Cresult : 0', df['A1Cresult'][df['A1Cresult'] == 0].count())\n",
    "\n",
    "# cleaning up the diag columns\n",
    "df = diagColn(df)\n",
    "\n",
    "print('diag_1 : 0', df['diag_1'][df['diag_1'] == 250.83].count())\n",
    "    \n",
    "# Run the normalizer on the dataframe\n",
    "# df_norm = scale(df, 'time_in_hospital')\n",
    "# df_norm = scale(df_norm, 'num_lab_procedures')\n",
    "# df_norm = scale(df_norm, 'num_procedures')\n",
    "# df_norm = scale(df_norm, 'num_medications')\n",
    "# df_norm = scale(df_norm, 'number_outpatient')\n",
    "# df_norm = scale(df_norm, 'number_emergency')\n",
    "# df_norm = scale(df_norm, 'number_inpatient')\n",
    "# df_norm\n",
    "\n",
    "drugEncoder = preprocessing.LabelEncoder()\n",
    "drugEncoder.fit(df['metformin'])\n",
    "for name,values in df.loc[:, 'metformin': 'metformin-pioglitazone'].iteritems():\n",
    "    df[name] = drugEncoder.transform(values)\n",
    "print(\"map of encoder: \" + str(list(drugEncoder.classes_)))\n",
    "\n",
    "\n",
    "df = transformLabelEncoder(df, 'change')\n",
    "df = transformLabelEncoder(df, 'diabetesMed')\n",
    "df = replaceColumnList(df, 'readmitted', ['>30','<30'], 1)\n",
    "df = replaceColumnList(df, 'readmitted', ['NO'], 0)\n",
    "\n",
    "df = transformLabelEncoder(df, 'gender')\n",
    "\n",
    "df.to_csv(\"dataCategorizedNew.csv\", sep=',', header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = pd.read_csv(\"dataCategorizedNew.csv\", nrows=1).columns\n",
    "\n",
    "Preprocessed_df_x = pd.read_csv(\"dataCategorizedNew.csv\", usecols=cols[:-1])\n",
    "Preprocessed_df_y = pd.read_csv(\"dataCategorizedNew.csv\", usecols=cols[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preprocessed_df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preprocessed_df_x.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preprocessed_df_x.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(Preprocessed_df_x, Preprocessed_df_y, test_size=0.20, random_state=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRclassifier = LogisticRegression(class_weight = 'balanced',random_state = 20, solver = 'liblinear',multi_class = 'ovr',verbose = 2,)\n",
    "LRclassifier.fit(X_train,Y_train)\n",
    "Y_predict = LRclassifier.predict(X_test)\n",
    "print(Y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "f1_score(Y_test, Y_predict, average='macro')\n",
    "accuracy_score(Y_test, Y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=0)\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_predict = clf.predict(X_test)\n",
    "\n",
    "print(f1_score(Y_test, Y_predict, average='macro'))\n",
    "print(accuracy_score(Y_test, Y_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
